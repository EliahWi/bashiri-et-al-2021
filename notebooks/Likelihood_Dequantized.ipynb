{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1> In this notebook we integrate the zero responses to get the likelihood for the original data. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:11:18.283514100Z",
     "start_time": "2023-11-16T20:11:18.282513300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from neuraldistributions.models import transforms\n",
    "from neuraldistributions.datasets import static\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:11:18.295512600Z",
     "start_time": "2023-11-16T20:11:18.289511900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from neuraldistributions.datasets import mouse_static_loaders, extract_data_key\n",
    "from neuraldistributions.models import poisson, zig, flowfa, ziffa, flowfa_ident\n",
    "from neuraldistributions.trainers import base_trainer\n",
    "from neuraldistributions.utility import get_loglikelihood\n",
    "\n",
    "from torch.distributions import LowRankMultivariateNormal\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:11:18.301512600Z",
     "start_time": "2023-11-16T20:11:18.295512600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_seed=42\n",
    "dataset_dir = \"../project/data\"\n",
    "datasets = [\n",
    "    \"static_edited_dsampled.zip\",\n",
    "]\n",
    "scan_id = [2, 1]\n",
    "dataset_paths = [f\"{dataset_dir}/{dataset}\" for dataset in datasets]\n",
    "areas = [[\"V1\", \"LM\"]]\n",
    "neurons_ns = [1000, 907]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:11:24.736459900Z",
     "start_time": "2023-11-16T20:11:18.322511700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5994/5994 [00:05<00:00, 1002.79it/s]\n",
      "100%|██████████| 5994/5994 [00:00<00:00, 624628.14it/s]\n",
      "100%|██████████| 5994/5994 [00:00<00:00, 636778.66it/s]\n",
      "100%|██████████| 5994/5994 [00:00<00:00, 667462.92it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_index = 0\n",
    "dataset_path = dataset_paths[dataset_index]\n",
    "data_key = extract_data_key(dataset_path)\n",
    "area = areas[dataset_index]\n",
    "neurons_n = neurons_ns[dataset_index]\n",
    "\n",
    "dataset_config = {\n",
    "    \"paths\": [dataset_path],\n",
    "    \"seed\": random_seed,\n",
    "    \"batch_size\": 64,\n",
    "    \"area\": area,\n",
    "    \"neurons_n\": neurons_n,\n",
    "    \"normalize_images\": True,\n",
    "    \"normalize_neurons\": True,\n",
    "    \"return_more\": True,\n",
    "    \"device\": device,\n",
    "    \"shuffle_train\": True,\n",
    "    \"return_real_responses\": False\n",
    "}\n",
    "\n",
    "dataloaders = mouse_static_loaders(**dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:17:19.044329900Z",
     "start_time": "2023-11-16T20:17:19.034329600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralpredictors.training import LongCycler\n",
    "def calcLossForDataset(model, dataset, neurons, samples_amount=100, in_bits=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = 0\n",
    "        samples_count = 0\n",
    "        for batch_idx, (data_key, batch) in enumerate(LongCycler(dataset)):\n",
    "            # data from batch\n",
    "            #print(batch_idx)\n",
    "            targets = batch[1]\n",
    "            images = batch[0]\n",
    "            \n",
    "            # latent for log_likelihood\n",
    "            mu = model.forward(images)\n",
    "            \n",
    "            if \"Ident\" in model.__class__.__name__:\n",
    "                dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, torch.eye(1000).to(mu.device))\n",
    "            else:\n",
    "                C, psi_diag = model.C_and_psi_diag\n",
    "                dist = LowRankMultivariateNormal(mu, C.T, psi_diag)\n",
    "            \n",
    "            # get zero and non zero responses\n",
    "            idx = targets <= torch.tensor(0)\n",
    "            n_idx = torch.logical_not(idx)\n",
    "            \n",
    "            \n",
    "            # calculate log_likelihood\n",
    "            samples = torch.FloatTensor(samples_amount, targets.shape[0], 1000).uniform_(-1, 0)\n",
    "            #importance_sample_dist = torch.distributions.exponential.Exponential(torch.tensor(4.5).expand(1000))\n",
    "            #samples = -importance_sample_dist.sample([samples_amount, targets.shape[0]])\n",
    "            samples[:,n_idx] = targets[n_idx].cpu()    \n",
    "        \n",
    "        \n",
    "            \n",
    "            transformed_targets, logdet = model.sample_transform(samples.to(device))\n",
    "            \n",
    "            log_likelihood = dist.log_prob(transformed_targets.detach()) + logdet.detach().sum(dim=2)\n",
    "            #print(log_likelihood.shape)\n",
    "            # for uniform samples between -x,0\n",
    "            loss_neurons = torch.logsumexp(log_likelihood,dim=0) - torch.log(torch.tensor(samples_amount).float()) + idx.sum(dim=1)*torch.log(torch.tensor(1.0))\n",
    "            loss = -torch.sum(loss_neurons)\n",
    "            \n",
    "            #print(loss_neurons.shape)\n",
    "            \n",
    "            # importance sampling\n",
    "            #loss = -torch.sum(torch.logsumexp(log_likelihood-importance_sample_dist.log_prob(-samples).to(device).sum(dim=2),dim=0) - torch.log(torch.tensor(samples_amount).float()))\n",
    "          \n",
    "            # the old loss for uniform samples between -1,0 (this might be wrong because of the log in the sum)\n",
    "            #loss = -torch.sum(torch.mean(log_likelihood, dim=0))\n",
    "            #print(loss)\n",
    "            #print(samples.shape)\n",
    "            #print(transformed_targets.shape)\n",
    "            #print(log_likelihood.shape)\n",
    "            #print(logdet.shape)\n",
    "            #print(dist.log_prob(transformed_targets.detach()).shape)\n",
    "            #return\n",
    "            losses += loss.item()\n",
    "            samples_count += len(batch[0])\n",
    "            del samples, transformed_targets, logdet, log_likelihood, targets, images\n",
    "\n",
    "        return losses / samples_count / neurons if in_bits==False else losses / samples_count / neurons / np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:17:20.564518300Z",
     "start_time": "2023-11-16T20:17:19.711912900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.167603544449458"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcLossForDataset(torch.load(\"./models/FlowFA\"), dataloaders[\"train\"], 1000, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-16T20:11:25.224115400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, file in enumerate((os.listdir(\"./models\"))):\n",
    "    if file == \"results.txt\" or file == \".ipynb_checkpoints\" or file==\"FlowFA-Original\":\n",
    "        continue\n",
    "    model = torch.load(f\"./models/{file}\")\n",
    "    print(file)\n",
    "    for samples in [10, 100, 1000, 5000]:\n",
    "        print(f\"\\tSample size: {samples}\")\n",
    "        print(f\"\\t\\tTrain loss: {calcLossForDataset(model, dataloaders['train'], 1000, samples,True)}\\n\\t\\tVal loss: {calcLossForDataset(model, dataloaders['validation'], 1000, samples,True)}\\n\\t\\tTest loss: {calcLossForDataset(model, dataloaders['test'], 1000, samples,True)}\")\n",
    "    del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
